<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="google-site-verification" content="" />
  
  <title>线性回归及python代码实现</title>
  <meta name="author" content="Zachary Zhang">
  <meta name="description" content="This is my personal blog, share knowledge, record life.">
  
  
  <meta property="og:title" content="线性回归及python代码实现"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:site_name" content="=just a blogs="/>
  <link href="/apple-touch-icon-precomposed.png" sizes="180x180" rel="apple-touch-icon-precomposed">
  <link rel="alternate" href="/atom.xml" title="=just a blogs=" type="application/atom+xml">
  <link rel="stylesheet" href="/css/m.min.css">
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <a id="top"></a>
  <div id="main">
    <div class="main-ctnr">
      <div class="behind">
  <a href="/" class="back black-color">
    <svg class="i-close" viewBox="0 0 32 32" width="22" height="22" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
        <path d="M2 30 L30 2 M30 30 L2 2"></path>
    </svg>
  </a>
  <div class="description">
    &nbsp;Hope to have what you need
  </div>
</div>


  <article class="standard post">
    <div class="title">
      
  
    <h1 class="page-title center">
        线性回归及python代码实现
    </h1>
  


    </div>
    <div class="meta center">
      <time datetime="2020-02-26T12:00:45.000Z" itemprop="datePublished">
  <svg class="i-calendar" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
    <path d="M2 6 L2 30 30 30 30 6 Z M2 15 L30 15 M7 3 L7 9 M13 3 L13 9 M19 3 L19 9 M25 3 L25 9"></path>
  </svg>
  &nbsp;
  2020-02-26
</time>


    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/categories/Machine-learning/">Machine learning</a>




    
    &nbsp;
    <svg class="i-tag" viewBox="0 0 32 32" width="16" height="16" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2">
      <circle cx="24" cy="8" r="2"></circle>
      <path d="M2 18 L18 2 30 2 30 14 14 30 Z"></path>
    </svg>
    &nbsp;
    <a href="/tags/line-regression/">line regression</a>·<a href="/tags/Python/">Python</a>


    </div>
    <hr>
    <div>
    
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90"><span class="toc-text">回归分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E6%8B%9F%E5%90%88"><span class="toc-text">函数拟合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-text">高斯分布</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0"><span class="toc-text">似然函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="toc-text">最小二乘法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-text">评估</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-python-%E6%A1%88%E4%BE%8B"><span class="toc-text">简单线性回归 python 案例</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sklearn-%E6%A1%88%E4%BE%8B"><span class="toc-text">sklearn 案例</span></a></li></ol>
    
    </div>
    <div class="picture-container">
      
    </div>
    <h1 id="回归分析">回归分析</h1>
<p>监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。</p>
<ul>
<li><strong>离散</strong>：连续的对应（就是反义词）就是离散 。离散就是不连续。例如像整数<code>1,2,3,4,5,...</code>这种就是离散的</li>
</ul>
<p>在统计学中，<strong>回归分析</strong>（<strong>regression analysis</strong>)指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。</p>
<p>在大数据分析中，<strong>回归分析</strong>是一种预测性的建模技术，它研究的是<strong>因变量</strong>（目标）和<strong>自变量</strong>（预测器）之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。</p>
<ol type="1">
<li>回归分析按照<strong>涉及的变量的多少</strong>，分为一元回归和多元回归分析；</li>
<li>按照<strong>因变量的多少</strong>，可分为简单回归分析和多重回归分析；</li>
<li>按照<strong>自变量和因变量之间的关系类型</strong>，可分为线性回归分析和非线性回归分析 <br></li>
</ol>
<h1 id="线性回归">线性回归</h1>
<p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为 <span class="math inline">\(y = w&#39;x+e\)</span>，其中 <span class="math inline">\(e\)</span> 为误差服从均值为 0 的正态分布。</p>
<ul>
<li>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。</li>
<li>如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。</li>
</ul>
<h4 id="例子">例子</h4>
<p>假如现在我想去找银行贷款，那我能不能在贷款之前先预测一下我能贷到多少钱呢？下面有这样一张表：</p>
<table>
<thead>
<tr class="header">
<th>工资</th>
<th>年龄</th>
<th>贷款额度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4000</td>
<td>25</td>
<td>20000</td>
</tr>
<tr class="even">
<td>8000</td>
<td>30</td>
<td>70000</td>
</tr>
<tr class="odd">
<td>5000</td>
<td>28</td>
<td>35000</td>
</tr>
<tr class="even">
<td>7500</td>
<td>33</td>
<td>50000</td>
</tr>
<tr class="odd">
<td>12000</td>
<td>40</td>
<td>85000</td>
</tr>
</tbody>
</table>
<p>通过这张表我们可以比较明显的看到工资、年龄与贷款额度之间是有一个关系的，大概是工资越高、年龄越小贷款的额度就相应的越高。那么我能不能根据自己的工资和年龄去预测一个具体的贷款值？</p>
<p>这里我们就可以引入回归分析了，我们可以把<strong>工资</strong>和<strong>年龄</strong>当作两个<strong>自变量</strong>，<strong>贷款额度</strong>当作<strong>因变量</strong>，通过求解自变量和因变量的线性关系，从而得到一个线性回归方程。利用得到的线性回归方程我们就可以进行预测了，那这种统计分析中利用线性回归方程对自变量和应变量建模的回归分析就叫做线性回归。 <br></p>
<h1 id="函数拟合">函数拟合</h1>
<p>继续上面的问题，现在我们把工资和年龄设为 <span class="math inline">\(x_1,x_2\)</span> (<strong>特征值</strong>)，贷款额度设为 <span class="math inline">\(h\)</span> (<strong>目标值</strong>)，去求一个线性函数，那么我们可以假设方程为：<span class="math inline">\(h=\alpha_0+\alpha_1x_1+\alpha_2x_2\)</span>，接着假设定义<span class="math inline">\(x_0\)</span>的值为 1，上面的式子可以表示成: <span class="math inline">\(h = \displaystyle\sum_{i=0}^n \alpha_i x_i=\alpha^ T X\)</span>。</p>
<p>实际情况中，变量之间不一定有线性关系，这时我们就要通过拟合的方式来解决。也就是找到一组最佳的参数 <span class="math inline">\(\alpha_0,\alpha_1,\alpha_2\)</span> 使得预测值最接近真实值。最后我们可能得到这样的结果：</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200225214559219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4NDEwNDk0,size_16,color_FFFFFF,t_70#pic_center" alt="dk-1" /><figcaption aria-hidden="true">dk-1</figcaption>
</figure>
<p>线性回归模型经常用<strong>最小二乘</strong>逼近来拟合，当然也可能用别的方法来拟合。 <br></p>
<h1 id="高斯分布">高斯分布</h1>
<p>通过上面的图，可以看出真实值和预测值之间一般是存在误差(用 <span class="math inline">\(\epsilon\)</span> 来表示误差)，于是(真实值) <span class="math inline">\(y =\displaystyle\sum_{i=0}^n \alpha_i x_i=\alpha^ T X+\epsilon\)</span>，对于每个样本有: <span class="math inline">\(y^{(i)} = \alpha^Tx^{(i)}+\epsilon^{(i)}\)</span>，误差<span class="math inline">\(\epsilon^{(i)}\)</span>是独立并且具有相同的分布，并且服从均值为 0，方差为<span class="math inline">\(\sigma^2\)</span>的高斯分布。</p>
<p>通常我们认为误差往往是很微小的，根据“<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86?fr=aladdin">中心极限定理</a>” ：在自然界与生产中，一些现象受到许多相互独立的随机因素的影响，如果每个因素所产生的影响都很微小时，总的影响可以看作是服从正态分布的。中心极限定理就是从数学上证明了这一现象 。所以高斯分布对误差假设来说是一种很好的模型。。</p>
<p>正态分布（Normal distribution），也称“常态分布”，又名高斯分布（Gaussian distribution）;正态曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，因此人们又经常称之为钟形曲线。</p>
<p>若随机变量<span class="math inline">\(X\)</span>服从一个数学期望为<span class="math inline">\(\mu\)</span>、方差为<span class="math inline">\(\sigma^2\)</span>的正态分布，记为<span class="math inline">\(N(\mu,\sigma^2)\)</span>。其概率密度函数为正态分布的期望值<span class="math inline">\(\mu\)</span>决定了其位置，其标准差<span class="math inline">\(\sigma\)</span>决定了分布的幅度。当<span class="math inline">\(\mu=0,\sigma=1\)</span>时的正态分布是标准正态分布。</p>
<p>通过概率密度函数来定义高斯分布：</p>
<p><img src="https://img-blog.csdnimg.cn/20200226004806895.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4NDEwNDk0,size_16,color_FFFFFF,t_70#pic_center" /></p>
<p>高斯分布的概率密度函数：<span class="math inline">\(p(y)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}\)</span></p>
<p>由于误差服从高斯分布，将其带入正太分布函数中(<span class="math inline">\(\mu=0\)</span>)，可以得到：<span class="math inline">\(p(y^{(i)}|x^{(i)};\alpha)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^{(i)} - \alpha ^Tx^{(i)})^2}{2\sigma^2}}\)</span> <br></p>
<h1 id="似然函数">似然函数</h1>
<p>在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。<strong>似然性</strong>是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。给定输出 x 时，关于参数 <span class="math inline">\(\theta\)</span> 的似然函数 <span class="math inline">\(L(\theta|x)\)</span>（在数值上）等于给定参数 <span class="math inline">\(\theta\)</span> 后变量 X 的概率：</p>
<p><span class="math display">\[L(\theta|x)=p(X=x|\theta)\]</span></p>
<p>似然函数的主要用法在于比较它相对取值，虽然这个数值本身不具备任何含义。例如，考虑一组样本，当其输出固定时，这组样本的某个未知参数往往会倾向于等于某个特定值，而不是随便的其他数，此时，似然函数是最大化的。</p>
<p><strong>最大似然估计</strong>：对同一个似然函数，如果存在一个参数值，使得它的函数值达到最大的话，那么这个值就是最为“合理”的参数值。是似然函数最初也是最自然的应用。在 <span class="math inline">\(\theta\)</span> 的所有取值上，使这个函数最大化。这个使可能性最大的值即被称为 <span class="math inline">\(\theta\)</span> 的最大似然估计。</p>
<p>现在我们要求解的就是最佳参数，也就是满足最大似然估计的 <span class="math inline">\(\alpha\)</span> ，所以将 <span class="math inline">\(p(y^{(i)}|x^{(i)};\alpha)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^{(i)} - \alpha ^Tx^{(i)})^2}{2\sigma^2}}\)</span>带入：</p>
<p><span class="math display">\[L(\alpha)=\displaystyle\prod_{i=1}^mp(y^{(i)}|x^{(i)};\alpha)=\displaystyle\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^{(i)} - \alpha ^Tx^{(i)})^2}{2\sigma^2}}\]</span></p>
<p><strong>对数似然</strong>：涉及到似然函数的许多应用中，更方便的是使用似然函数的自然对数形式，即“对数似然函数”。求解一个函数的极大化往往需要求解该函数的关于未知参数的偏导数。由于对数函数是单调递增的，而且对数似然函数在极大化求解时较为方便，所以对数似然函数常用在最大似然估计及相关领域中。</p>
<p><span class="math display">\[\log  L(\alpha)=\log \displaystyle\prod_{i=1}^mp(y^{(i)}|x^{(i)};\alpha)=\log \displaystyle\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^{(i)} - \alpha^T x^{(i)})^2}{2\sigma^2}}\]</span></p>
<p><span class="math display">\[= \displaystyle\sum_{i=1}^m\log\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y^i - \alpha^T x^i)^2}{2\sigma^2}}\]</span></p>
<p><span class="math display">\[\quad \quad =m\log \frac{1}{\sigma\sqrt{2\pi}}-\frac{1}{\sigma^2}\cdot\frac{1}{2}\displaystyle\sum_{i=1}^m(y^{(i)} - \alpha^T x^{(i)})^2\]</span></p>
<p>目标是要使的对数似然函数最大，<span class="math inline">\(m\log \frac{1}{\sigma\sqrt{2\pi}}\)</span>是一个常数，所以就得让后面减去的最小，得到目标函数：</p>
<p><span class="math display">\[J(\alpha)=\frac{1}{2}\displaystyle\sum_{i=1}^m(y^{(i)} - \alpha^T x^{(i)})^2=\frac{1}{2}\displaystyle\sum_{i=1}^m(Y(x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\alpha-y)^T(X\alpha-y) \]</span></p>
<p>对 <span class="math inline">\(\alpha\)</span> 求偏导：</p>
<p><span class="math display">\[\nabla_\alpha J(\alpha)=\nabla_\alpha(\frac{1}{2}(X\alpha-y)^T(X\alpha-y))=\nabla_\alpha(\frac{1}{2}(\alpha^TX^T-y^T)(X\alpha-y))\]</span></p>
<p><span class="math display">\[=\nabla_\alpha\frac{1}{2}(\alpha^TX^TX\alpha-\alpha^TX^Ty-y^TX\alpha-y+y^Ty))\]</span></p>
<p><span class="math display">\[ =\frac{1}{2}(2X^TX\alpha-X^Ty-(y^TX)^T)=X^TX\alpha-X^Ty\]</span></p>
<p>令偏导等于 0：</p>
<p><span class="math display">\[\alpha=(X^TX)^{-1}X^Ty\quad\quad（最小二乘）\]</span> <br></p>
<h1 id="最小二乘法">最小二乘法</h1>
<p>假如我们在研究两个变量<span class="math inline">\(（x,y）\)</span>之间的相互关系时，通常可以得到一系列成对的数据<span class="math inline">\(（x1,y1.x2,y2... xm,ym）\)</span>；将这些数据描绘在 <code>x-y</code> 直角坐标系中，若发现这些点在一条直线附近,可以设这条直线方程为：<span class="math inline">\(y_i=a_0+a_1x(其中 a_0、a_1任意实数)\)</span>。</p>
<p>为了建立这条直线，那么我们就要确定<span class="math inline">\(a_0\)</span>和<span class="math inline">\(a_1\)</span>的值，我们将实际值 <span class="math inline">\(y_i\)</span>与计算值 <span class="math inline">\(y_j(y_j=a_0+a_1x )\quad[1]\)</span> 的差<span class="math inline">\((y_i-y_j)\)</span>的平方和 <span class="math inline">\(\varphi=\displaystyle\sum_{i=1}^{n} (y_i-y_j)^2\quad[2]\)</span> 最小作为<strong>优化判据</strong>。</p>
<p>我们把[1]式带入[2]式中，得到 <span class="math inline">\(\varphi=\displaystyle\sum_{i=1}^{n} (y_i-a_0-a_1x_i )^2 \quad[3]\)</span></p>
<p>为了使 <span class="math inline">\(\varphi\)</span> 的值最小，我们可以通过求解最小值的方法，让函数 <span class="math inline">\(\varphi\)</span> 对 <span class="math inline">\(a_0、a_1\)</span> 求偏导，令两个偏导数等于零。这样我们就的到了式[4]和式[5]。</p>
<p><span class="math display">\[\begin{cases}\displaystyle\sum_{i=1}^{n} 2(a_0+a_1x_i-y_i )=0 \quad[4]\\\displaystyle\sum_{i=1}^{n} 2x_i(a_0+a_1x_i-y_i )=0 \quad[5]\end{cases}\]</span></p>
<p>求解方程组，可以得到 <span class="math inline">\(a_0=\frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-a_1x_i) , a_1=\frac{n\displaystyle\sum_{i=1}^nx_iy_i-\displaystyle\sum_{i=1}^nx_i\displaystyle\sum_{i=1}^ny_i}{n\displaystyle\sum_{i=1}^nx_i^2-\displaystyle\sum_{i=1}^nx_i\displaystyle\sum_{i=1}^nx_i}\)</span> ，将 <span class="math inline">\(a_0\)</span> 和 <span class="math inline">\(a_1\)</span> 带入[1]式中就的到了我们的一元线性方程，即：数学模型。</p>
<p>这里以最简单的一元线性模型来解释最小二乘法。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面。 <br></p>
<h1 id="梯度下降">梯度下降</h1>
<p>梯度下降是迭代法的一种,沿着这个函数下降的方向找，逼近最小值,可以用于求解最小二乘问题。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。</p>
<p>在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。</p>
<p>其迭代公式为 <span class="math inline">\(a_{k+1}=a_k+\rho k^{\bar{s}^{(k)}}\)</span> ,其中 <span class="math inline">\(\bar{s}^{(k)}\)</span> 代表梯度负方向，<span class="math inline">\(\rho k\)</span> 表示梯度方向上的搜索步长。梯度方向我们可以通过对函数求导得到，步长的确定比较麻烦，太大了的话可能会发散，太小收敛速度又太慢。 <br></p>
<h1 id="评估">评估</h1>
<p>通常使用 RSE（残差标准差）和 R 来评估模型</p>
<p><span class="math inline">\(R^2=1-\frac{RSS}{TSS}=1-\frac{\displaystyle\sum_{i=1}^m(\hat{y_i}-y_i)^2}{\displaystyle\sum_{i=1}^m(y_i-\bar{y})^2}\quad\quad\quad\quad \frac{残差平方和}{类似方差项}\)</span></p>
<p><span class="math inline">\(R^2\)</span> 可以衡量因变量的变化比例，并用自变量 <span class="math inline">\(x\)</span> 表述。因此，假设在一个线性方程中，自变量 <span class="math inline">\(x\)</span> 可以解释因变量，那么变化比例就高， <span class="math inline">\(R^2\)</span> 将接近 1。反之，则接近 0。所以 <span class="math inline">\(R^2\)</span>的取值越接近 1 ，我们就认为模型拟合的与好 <br></p>
<h1 id="简单线性回归-python-案例">简单线性回归 python 案例</h1>
<p>导入用到的包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>生成数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置随机生成算法的初始值</span></span><br><span class="line">np.random.seed(<span class="number">300</span>)</span><br><span class="line">data_size = <span class="number">100</span></span><br><span class="line"><span class="comment">#  生出size个符合均分布的浮点数，取值范围为[low, high)，默认取值范围为[0, 1.0)</span></span><br><span class="line">x = np.random.uniform(low=<span class="number">1.0</span>, high=<span class="number">10.0</span>, size=data_size)</span><br><span class="line">y = x * <span class="number">20</span> + <span class="number">10</span> + np.random.normal(loc=<span class="number">0.0</span>, scale=<span class="number">10.0</span>, size=data_size)</span><br></pre></td></tr></table></figure>
<p>查看数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(x)</span><br><span class="line">print(y.shape)</span><br><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>[5.06010312 2.98920108 4.32173127 3.61698133 3.16970135 8.0384179
 8.29984929 4.60711487 1.23399282 6.48435743 6.66179289 6.86728789
 8.72731994 6.28122472 7.33751513 5.37311508 7.15974709 2.01131411
 2.64152172 7.27343316 4.31517483 4.84567523 4.59836046 6.83840906
 4.7201361  2.47241009 5.57532693 5.65184176 4.13384671 3.72027221
 3.04603181 1.49130937 7.68238581 3.78143155 9.79455216 4.27478265
 5.42937585 7.76009388 5.53962905 2.12052548 8.35322843 4.32938243
 3.18827513 8.28250053 3.09192118 3.2946656  4.92652317 8.48630166
 6.78218775 7.45073433 7.36576515 7.96927182 4.13018028 7.03576017
 6.43589797 7.3106855  9.21539573 9.0112781  9.5537087  7.9801062
 5.44528565 6.62427759 4.46709413 5.58802991 8.67279532 3.2084603
 9.61530432 8.82526244 2.18656259 5.71768665 7.5517145  3.89302569
 8.74359262 2.08295081 5.91832925 5.46471639 9.43168746 1.58599839
 7.63747752 4.87551191 8.56518562 7.88432931 3.48748529 1.72525194
 6.83057399 2.76268027 6.48435105 8.16047282 2.30107294 2.63178986
 8.74522455 5.82819158 6.58947199 5.36626558 4.85639013 6.27787997
 3.56740106 7.88396527 1.66851535 4.97195607]
(100,)</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20200226164326462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4NDEwNDk0,size_16,color_FFFFFF,t_70" /></p>
<p>区分训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个随机排列</span></span><br><span class="line">shuffled_index = np.random.(data_size)</span><br><span class="line">x = x[shuffled_index]</span><br><span class="line">y = y[shuffled_index]</span><br><span class="line">split_index = <span class="built_in">int</span>(data_size * <span class="number">0.75</span>)</span><br><span class="line">x_train = x[:split_index]</span><br><span class="line">y_train = y[:split_index]</span><br><span class="line">x_test = x[split_index:]</span><br><span class="line">y_test = y[split_index:]</span><br></pre></td></tr></table></figure>
<p>线性回归类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinerRegression</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">100</span>, seed=<span class="literal">None</span></span>):</span></span><br><span class="line">        np.random.seed(seed)</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.a = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.b = np.random.normal(<span class="number">1</span>, <span class="number">0.1</span>)</span><br><span class="line">        self.loss_arr = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            self._train_step()</span><br><span class="line">            self.loss_arr.append(self.loss())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_f</span>(<span class="params">self, x, a, b</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;一元线性函数&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x * a + b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;预测&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> x <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.x</span><br><span class="line">        y_pred = self._f(x, self.a, self.b)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, y_true=<span class="literal">None</span>, y_pred=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;损失&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> y_true <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> y_pred <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            y_true = self.y</span><br><span class="line">            y_pred = self.predict(self.x)</span><br><span class="line">        <span class="keyword">return</span> np.mean((y_true - y_pred)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_calc_gradient</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;梯度&quot;&quot;&quot;</span></span><br><span class="line">        d_a = np.mean((self.x * self.a + self.b - self.y) * self.x)</span><br><span class="line">        d_b = np.mean(self.x * self.a + self.b - self.y)</span><br><span class="line">        print(d_a, d_b)</span><br><span class="line">        <span class="keyword">return</span> d_a, d_b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_train_step</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;训练频度&quot;&quot;&quot;</span></span><br><span class="line">        d_a, d_b = self._calc_gradient()</span><br><span class="line">        self.a = self.a - self.lr * d_a</span><br><span class="line">        self.b = self.b - self.lr * d_b</span><br><span class="line">        <span class="keyword">return</span> self.a, self.b</span><br></pre></td></tr></table></figure>
<p>训练数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regr = LinerRegression(learning_rate=<span class="number">0.01</span>, max_iter=<span class="number">10</span>, seed=<span class="number">314</span>)</span><br><span class="line">regr.fit(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p>展示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f&#x27;cost: \t<span class="subst">&#123;regr.loss():<span class="number">.3</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&quot;f=<span class="subst">&#123;regr.a:<span class="number">.2</span>&#125;</span> x + <span class="subst">&#123;regr.b:<span class="number">.2</span>&#125;</span>&quot;</span>)</span><br><span class="line">plt.scatter(np.arange(<span class="built_in">len</span>(regr.loss_arr)), regr.loss_arr, marker=<span class="string">&#x27;o&#x27;</span>, c=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>cost:   1.11e+02
f=2.1e+01 x+4.2</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20200226180220573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4NDEwNDk0,size_16,color_FFFFFF,t_70" /></p>
<p><img src="https://img-blog.csdnimg.cn/20200226181102306.gif" /></p>
<p>评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">y_pred = regr.predict(x_test)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sqrt_R</span>(<span class="params">y_pred, y_test</span>):</span></span><br><span class="line">    y_c = y_pred - y_test</span><br><span class="line">    rss = np.mean(y_c ** <span class="number">2</span>)</span><br><span class="line">    y_t = y_test - np.mean(y_test)</span><br><span class="line">    tss = np.mean(y_t ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> (<span class="number">1</span> - rss / tss)</span><br><span class="line">r = sqrt_R(y_pred, y_test)</span><br><span class="line">print(<span class="string">f&quot;R: <span class="subst">&#123;r&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>R: 0.9327511073300032</code></pre>
<p><br></p>
<h1 id="sklearn-案例">sklearn 案例</h1>
<p>导包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor, Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br></pre></td></tr></table></figure>
<p>载入数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;线性回归预测房子价格&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">lb = load_boston()</span><br><span class="line">print(lb)</span><br></pre></td></tr></table></figure>
<p>处理数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割数据集 （训练集、测试集）</span></span><br><span class="line">x_train, x_text, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>)</span><br><span class="line"><span class="comment"># 进行标准化</span></span><br><span class="line"><span class="comment"># 特征值 目标值 都标准化</span></span><br><span class="line">std_x = StandardScaler()</span><br><span class="line">x_train = std_x.fit_transform(x_train)</span><br><span class="line">x_text = std_x.fit_transform(x_text)</span><br><span class="line"></span><br><span class="line">std_y = StandardScaler()</span><br><span class="line">y_train = std_y.fit_transform(y_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 传入数据必须二维</span></span><br><span class="line">y_test = std_y.fit_transform(y_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>方程求解</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># estimator预测</span></span><br><span class="line"><span class="comment"># 正规方程求解方式</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(lr.coef_)</span></span><br><span class="line"></span><br><span class="line">y_lr_predict = std_y.inverse_transform(lr.predict(x_text))</span><br><span class="line"><span class="comment"># print(&quot;预测：\n&quot;, y_lr_predict)</span></span><br><span class="line">print(<span class="string">&quot;均方误差：&quot;</span>, mean_squared_error(std_y.inverse_transform(y_test), y_lr_predict))</span><br><span class="line">print(<span class="string">f&quot;R : <span class="subst">&#123;lr.score(x_test, y_test)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[[-0.10633041  0.10911523  0.00409775  0.06205962 -0.23187842  0.29700813
  -0.00083156 -0.32863065  0.2570794  -0.18807137 -0.22742163  0.10321854
  -0.39041034]
均方误差： 21.92038221080247
R : 0.6956028823910707</code></pre>
<p>梯度下降</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降方式</span></span><br><span class="line">sgd = SGDRegressor()</span><br><span class="line">sgd.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">y_sgd_predict = std_y.inverse_transform(sgd.predict(x_text))</span><br><span class="line">print(y_sgd_predict)</span><br><span class="line">print(<span class="string">&quot;均方误差：&quot;</span>, mean_squared_error(std_y.inverse_transform(y_test), y_sgd_predict))</span><br><span class="line">print(<span class="string">f&quot;R : <span class="subst">&#123;sgd.score(x_test, y_test)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>均方误差： 17285.95206027733
R : 0.6907551016044395</code></pre>


  </article>
  </script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>
  <div class="busuanzi center">
    page PV:&nbsp;<span id="busuanzi_value_page_pv"></span>&nbsp;・&nbsp;
    site PV:&nbsp;<span id="busuanzi_value_site_pv"></span>&nbsp;・&nbsp;
    site UV:&nbsp;<span id="busuanzi_value_site_uv"></span>
  </div>


    





    </div>
  </div>
  <footer class="page-footer"><div class="clearfix">
</div>
<div class="right-foot">
    <div class="firstrow">
        <a href="#top" target="_self">
        <svg class="i-caret-right" viewBox="0 0 32 32" width="24" height="24" fill="none" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="3">
            <path d="M10 30 L26 16 10 2 Z"></path>
        </svg>
        </a>
        © zachary 2018-2021
    </div>
    <div class="secondrow">
        <a target="_blank" rel="noopener" href="https://github.com/gaoryrt/hexo-theme-pln">
        Theme Pln
        </a>
    </div>
</div>
<div class="clearfix">
</div>
</footer>
  <script src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script>
<script src="/js/search.min.js"></script>
<script type="text/javascript">

// disqus scripts


// dropdown scripts
$(".dropdown").click(function(event) {
  var current = $(this);
  event.stopPropagation();
  $(current).children(".dropdown-content")[($(current).children(".dropdown-content").hasClass("open"))?'removeClass':'addClass']("open")
});
$(document).click(function(){
    $(".dropdown-content").removeClass("open");
})

var path = "/search.xml";
searchFunc(path, 'local-search-input', 'local-search-result');

</script>

</body>
</html>
